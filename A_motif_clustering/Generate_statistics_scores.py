import os
import csv
import pandas as pd

# Path to the folder containing the output final_cluster.tsv files from running the .... .sh script 
folder_path = "/Users/juliaancker/Desktop/full_data_random_clusters"

# Ensure the folder exists
if not os.path.exists(folder_path):
    print(f"Error: Folder '{folder_path}' does not exist.")
    exit()

# Initialize an empty dictionary
cluster_data = {}

# Loop through all files in the folder
for file_name in os.listdir(folder_path):
    file_path = os.path.join(folder_path, file_name)
    
    # Check if it's a file (skip directories)
    if os.path.isfile(file_path):
        
        # Open the file and process it
        with open(file_path, 'r') as file:
            csv_reader = csv.DictReader(file, delimiter='\t')  # Output files are tsv files from Hammock script

            for row in csv_reader:
                # Skip rows with "CCCCC" in the 'main_sequence' column as these sequences were oly used to keep all labels
                if row.get('main_sequence') == "CCCCC":
                    continue
                
                # Iterate over columns starting from the first column with id labels
                for column_name in list(row.keys())[4:]:
                    value = row[column_name]
                    
                    # Skip cells with the value 0
                    if value == '0':
                        continue
                    
                    # If the cell is not 0 get the sequence id (column_name) and add count
                    cluster_data[column_name] = cluster_data.get(column_name, 0) + int(value)

# Print the resulting dictionary

# Path to folder with the sequences that were identical to a random sequence and removed to run the algorithm
duplicates_file = "/Users/juliaancker/Desktop/non_matching_sequences.fa"



# Process each .fa file in the folder
with open(duplicates_file, 'r') as fasta_file:
    for line in fasta_file:
        line = line.strip()
        if line.startswith(">"):  # Header line
                    # Extract the first number before the first '|'
            header_number = line.split('|')[0].lstrip(">")
                    
                    # Add a count to each sequence that had a identical random sequence as they would cluster together
            cluster_data[header_number] = cluster_data.get(header_number, 0) + 1


# Extract the information about which cluster each sequence that have clustered to a random cluster belongs to

sequence_path = "/Users/juliaancker/Desktop/projekt_bioinf/results/full_set/final_clusters_sequences.tsv" # Path to the final_clusters_sequences.tsv generated by the algorithm for the original run. 
data_set = "20241122_motif_mapped_data.xlsx"

clusterid_seq = {}

df_data = pd.read_excel(data_set)
df_sequences = pd.read_csv(sequence_path, sep='\t')

# Itterate through the original dataset
for _, row in df_data.iterrows():
    id = row['Id']
    sequence = row['new_motif'] # Get the sequence
    cluster_id = df_sequences.loc[df_sequences['sequence'] == sequence, 'cluster_id'].values # Find the sequence in the final_cluster_sequences.tsv file and get the cluster id
    clusterid_seq[id] = cluster_id


cluster_random_data = {}

# Count how many times sequences from each cluster was found in a random cluster

for seq, count in cluster_data.items():

    cluster_id = clusterid_seq[int(seq)] # From a sequence id get the cluster id it belongs to
    c_id = cluster_id[0] if len(cluster_id) > 0 else None
    if c_id in cluster_random_data and c_id != None:
        cluster_random_data[c_id] += count # Add the count for a sequence to the cluster it belongs to
    else:
        cluster_random_data[c_id] = count # Add the count for a sequence to the cluster it belongs to

# Total number of unique sequences in each cluster
total_unique = {1760: 154, 1805: 194, 1860: 88, 1890: 84,197: 2, 1989: 106, 2066: 134, 2102: 118, 2156: 76, 2213: 124, 2282: 110, 2304: 156, 2336: 92, 2337: 98, 2384: 56, 2442: 134, 2468: 68}

norm_scores = {}

# Normalise based on unique sequences 
for c_id, score in cluster_random_data.items():
    if c_id:
        norm_scores[c_id] = score / (total_unique[c_id] * 108)


for key, value in norm_scores.items():
    print(f'{key}: {value}')

